{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_users=19244, n_items=14596\n",
      "n_interactions=135326\n",
      "n_train=95629, n_val=20127, n_test=19570, sparsity=0.00048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19244/19244 [01:10<00:00, 273.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text 0.10991018 0.086630285\n",
      "img 0.3009104 0.23256181\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# avg.sim (Figure 1)\n",
    "\n",
    "import json\n",
    "import os\n",
    "from utility.load_data import Data\n",
    "\n",
    "data_generator = Data(path='data/WomenClothing', batch_size=1024)\n",
    "\n",
    "from copy import deepcopy\n",
    "I_items = deepcopy(data_generator.train_items)\n",
    "\n",
    "for k in I_items.keys():\n",
    "    I_items[k] = I_items[k] + data_generator.val_set[k] + data_generator.test_set[k]\n",
    "\n",
    "import numpy as np\n",
    "image_feats = np.load('data/WomenClothing/image_feat.npy')\n",
    "text_feats = np.load('data/WomenClothing/text_feat.npy')\n",
    "\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "img_cos = np.dot(image_feats, image_feats.T) / (np.linalg.norm(image_feats, axis=1)[:, np.newaxis] * np.linalg.norm(image_feats, axis=1)[:, np.newaxis].T)\n",
    "text_cos = np.dot(text_feats, text_feats.T) / (np.linalg.norm(text_feats, axis=1)[:, np.newaxis] * np.linalg.norm(text_feats, axis=1)[:, np.newaxis].T)\n",
    "\n",
    "seen_img = []\n",
    "seen_text = []\n",
    "unseen_img = []\n",
    "unseen_text = []\n",
    "for user, items in tqdm(I_items.items()):\n",
    "    img = img_cos[items][:, items]\n",
    "    text = text_cos[items][:, items]\n",
    "\n",
    "    seen_img_result = []\n",
    "    seen_text_result = []\n",
    "    for i in range(len(items)):\n",
    "        seen_img_result.append(np.concatenate([img[i, :i], img[i, i+1:]]))\n",
    "        seen_text_result.append(np.concatenate([text[i, :i], text[i, i+1:]]))\n",
    "    seen_img_result = np.array(seen_img_result) # .flatten()\n",
    "    seen_text_result = np.array(seen_text_result) # .flatten()\n",
    "\n",
    "    unseen_items = set(range(data_generator.n_items)) - set(items)\n",
    "    unseen_items = list(unseen_items)\n",
    "\n",
    "    unseen_img_result = img_cos[items][:, unseen_items].flatten()\n",
    "    unseen_text_result = text_cos[items][:, unseen_items].flatten()\n",
    "\n",
    "    seen_img.append(seen_img_result.mean())\n",
    "    seen_text.append(seen_text_result.mean())\n",
    "    unseen_img.append(unseen_img_result.mean())\n",
    "    unseen_text.append(unseen_text_result.mean())\n",
    "\n",
    "print('text', np.mean(seen_text), np.mean(unseen_text))\n",
    "print('img', np.mean(seen_img), np.mean(unseen_img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/yg/lib/python3.6/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_users=19244, n_items=14596\n",
      "n_interactions=135326\n",
      "n_train=95629, n_val=20127, n_test=19570, sparsity=0.00048\n",
      "Loads image_emb: torch.Size([33840, 64]) and text_emb: torch.Size([33840, 64])\n",
      "0.14228745 0.11385205\n",
      "Loads image_emb: torch.Size([33840, 64]) and text_emb: torch.Size([33840, 64])\n",
      "0.3034921 0.10677319\n",
      "Loads image_emb: torch.Size([33840, 64]) and text_emb: torch.Size([33840, 64])\n",
      "0.3312145 0.1141393\n",
      "Loads image_emb: torch.Size([33840, 64]) and text_emb: torch.Size([33840, 64])\n",
      "0.17027126 0.110791825\n",
      "Loads image_emb: torch.Size([33840, 64]) and text_emb: torch.Size([33840, 64])\n",
      "0.2663411 0.11235878\n"
     ]
    }
   ],
   "source": [
    "# avg.diff\n",
    "from Models import *\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '1'\n",
    "import torch\n",
    "import numpy as np\n",
    "from utility.load_data import Data\n",
    "data_generator = Data(path='data/WomenClothing', batch_size=1024)\n",
    "\n",
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    indices = torch.from_numpy(\n",
    "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "    values = torch.from_numpy(sparse_mx.data)\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)\n",
    "\n",
    "model_name = 'MONET_concat_20_03'\n",
    "seed_list = ['123', '0', '42', '1024', '2048']\n",
    "nonzero_idx = data_generator.nonzero_idx()\n",
    "\n",
    "import numpy as np\n",
    "image_feats = np.load('data/WomenClothing/image_feat.npy')\n",
    "text_feats = np.load('data/WomenClothing/text_feat.npy')\n",
    "    \n",
    "for seed in seed_list:    \n",
    "    model = MONET(data_generator.n_users, data_generator.n_items, 64, nonzero_idx, True, image_feats, text_feats, 2, 1.0, 0.3, 'concat', 's', False)    \n",
    "    model.load_state_dict(torch.load('./models/' + 'WomenClothing' + '_' + model_name + '_' + seed, map_location=torch.device('cpu'))[model_name + '_' + seed])\n",
    "    model.to(device)\n",
    "    image_emb, text_emb = model(eval=True)\n",
    "    print('Loads image_emb: {} and text_emb: {}'.format(image_emb.shape, text_emb.shape))\n",
    "\n",
    "    # user_emb = torch.load('data/{}/{}_user_emb.pt'.format('clothing', 'lightgcn_layer3_original')).to(device)\n",
    "    # item_emb = torch.load('data/{}/{}_item_emb.pt'.format('clothing', 'lightgcn_layer3_original')).to(device)\n",
    "    # print('Loads user_emb: {} and item_emb: {}'.format(user_emb.weight.shape, item_emb.weight.shape))\n",
    "\n",
    "    # image_emb = image_emb.mean(dim=1, keepdim=False)\n",
    "    # text_emb = text_emb.mean(dim=1, keepdim=False)\n",
    "\n",
    "    # image_emb = image_emb[:, -1, :]\n",
    "    # text_emb = text_emb[:, -1, :]\n",
    "\n",
    "\n",
    "    final_image_preference, final_image_emb = torch.split(image_emb, [data_generator.n_users, data_generator.n_items], dim=0)\n",
    "    final_text_preference, final_text_emb = torch.split(text_emb, [data_generator.n_users, data_generator.n_items], dim=0)\n",
    "\n",
    "    final_text_emb, final_image_emb = final_text_emb.cpu().detach().numpy(), final_image_emb.cpu().detach().numpy()\n",
    "\n",
    "    final_image_cos = np.dot(final_image_emb, final_image_emb.T) / (np.linalg.norm(final_image_emb, axis=1)[:, np.newaxis] * np.linalg.norm(final_image_emb, axis=1)[:, np.newaxis].T)\n",
    "    final_text_cos = np.dot(final_text_emb, final_text_emb.T) / (np.linalg.norm(final_text_emb, axis=1)[:, np.newaxis] * np.linalg.norm(final_text_emb, axis=1)[:, np.newaxis].T)\n",
    "\n",
    "    img_cos = np.dot(image_feats, image_feats.T) / (np.linalg.norm(image_feats, axis=1)[:, np.newaxis] * np.linalg.norm(image_feats, axis=1)[:, np.newaxis].T)\n",
    "    text_cos = np.dot(text_feats, text_feats.T) / (np.linalg.norm(text_feats, axis=1)[:, np.newaxis] * np.linalg.norm(text_feats, axis=1)[:, np.newaxis].T)\n",
    "\n",
    "    img_diff = np.abs(img_cos - final_image_cos)\n",
    "    text_diff = np.abs(text_cos - final_text_cos)\n",
    "\n",
    "    img = []\n",
    "    for i in range(data_generator.n_items):\n",
    "        img.append(np.concatenate([img_diff[i, :i], img_diff[i, i+1:]]))\n",
    "    img = np.array(img) # .flatten()\n",
    "\n",
    "    txt = []\n",
    "    for i in range(data_generator.n_items):\n",
    "        txt.append(np.concatenate([text_diff[i, :i], text_diff[i, i+1:]]))\n",
    "    txt = np.array(txt) # .flatten()\n",
    "\n",
    "    print(img[~np.isnan(img)].mean(), txt[~np.isnan(txt)].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0aa7af790e1209bd084877485dad105a461ac2ebd38ac99cff72d3e7c0921c3c"
  },
  "kernelspec": {
   "display_name": "yg",
   "language": "python",
   "name": "yg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13 |Anaconda, Inc.| (default, Jun  4 2021, 14:25:59) \n[GCC 7.5.0]"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
